{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[str: \"\" (1,3), eof: $ (1,3)]\n"
     ]
    }
   ],
   "source": [
    "from Lexer.lexer import Lexer\n",
    "\n",
    "\n",
    "\n",
    "nonzero_digits = '|'.join(str(n) for n in range(1,10))\n",
    "letters = '|'.join(chr(n) for n in range(ord('a'),ord('z')+1))\n",
    "upper_letters= '|'.join(chr(n) for n in range(ord('A'),ord('Z')+1))\n",
    "num1=f'({nonzero_digits})(0|{nonzero_digits})*.(0|{nonzero_digits})(0|{nonzero_digits})*'\n",
    "num2=f'({nonzero_digits})(0|{nonzero_digits})*'\n",
    "num3=f'0.(0|{nonzero_digits})(0|{nonzero_digits})*'\n",
    "number = '|'.join(n for n in [num1,num2,num3,'0'])   \n",
    "symbols='!|@|%|^|&|_|+|-|:|;|<|>|=|,|.|?|~|`|[|]|{|}|#|¿|¡|º|ª|¬'\n",
    "string= f'(\")({symbols}|0| |{nonzero_digits}|{letters}|{upper_letters})*(\")'  \n",
    "\n",
    "lexer = Lexer([\n",
    "    ('str', string),\n",
    "    # ('for' , 'for'),\n",
    "    # ('foreach' , 'foreach'),\n",
    "    ('space', '  *')\n",
    "    # ('id', f'({letters})({letters}|0|{nonzero_digits})*')\n",
    "], 'eof')\n",
    "\n",
    "text= '\"\"'\n",
    "tokens1 = lexer(text)\n",
    "print(tokens1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# text = f\"5 for 4foreach {'\\n'} fore\"\n",
    "\n",
    "# tokens = lexer(text)\n",
    "\n",
    "# print(tokens)\n",
    "# assert [t.token_type for t in tokens] == ['num', 'space', 'for', 'space', 'num', 'foreach', 'space', 'id', 'eof']\n",
    "# assert [t.lex for t in tokens] == ['5465', ' ', 'for', ' ', '45', 'foreach', ' ', 'fore', '$']\n",
    "\n",
    "# text = '4forense forforeach for4foreach foreach 4for'\n",
    "# print(f'\\n>>> Tokenizando: \"{text}\"')\n",
    "# tokens = lexer(text)\n",
    "# print(tokens)\n",
    "# assert [t.token_type for t in tokens] == ['num', 'id', 'space', 'id', 'space', 'id', 'space', 'foreach', 'space', 'num', 'for', 'eof']\n",
    "# assert [t.lex for t in tokens] == ['4', 'forense', ' ', 'forforeach', ' ', 'for4foreach', ' ', 'foreach', ' ', '4', 'for', '$']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
